

% \begin{table*}[t]
% \begin{center}
%     \begin{minipage}[b]{1\textwidth}
% \begin{tabular}{lrrrrrr}
% \toprule
% & \multicolumn{3}{c}{Train} & \multicolumn{3}{c}{Test}\\
% Methods  & Acc & Edit & F1$@\{10,25,50\}$ & Acc & Edit & F1$@\{10,25,50\}$ \\
% \midrule
% FC  & 44.00 & 26.71 & 12.42~~22.64~~19.40 & 34.90 & 18.58 & 17.47~~13.66~~8.04\\
% % EDTCN \cite{8099596} & & & & & & \\
% MS-TCN \cite{8953830} & 43.52 & & & 38.65 & & \\
% % RNN+HMM \cite{8099623} & & & & & & \\
% DTGRM \cite{wang2020temporal} & 52.58 & & & 37.71 & & \\
% \midrule
% Proposed Method (\textit{narration})           & & & & \\
% Proposed Method (\textit{narration+context})           & & & & \\
% \bottomrule
% \end{tabular}
% \caption{Results of baseline models}
% \label{table:results}
% \end{minipage}
% \end{center}
% \end{table*}





\subsection{HowTo100M Experiments} \label{section:howto100m-experiments}
We conducted experiments to evaluate the quality of the video-text joint-embedding learnt after finetuning the HowTo100M model. For a video which is made up from $N$ segments, we use the ground truth start and end frame number, $(s_i,e_i)$, to segment out the $i$-th actions. Following the feature extraction procedure described in Section~\ref{section:video-text-matching}, we extract visual feature $\mathbf{v}_i$ for the $i$-th segment. In order to test what kinds of text input is helpful in building the joint-embedding, we consider two kinds of input in describing the action in a given segment and use the word2vec model mentioned in \ref{section:video-text-matching} to extract the text embeddings: embedding $\mathbf{c}^{verb}_i$ denoting single action verb, and $\mathbf{c}^{narr}_i$ denoting entire narration including verb and noun. 

In Table~\ref{table:howto100m_seg_threshold} and Table~\ref{table:howto100m_label}, the \textit{Label} column shows different $\mathbf{c}{'}_i$ used to calculate the similarity score $s(\mathbf{v}{'}_i, \mathbf{c}{'}_i)$ for video-text retrieval. For 
\textit{Narration}, $\mathbf{c}{'}_i$ is $\mathbf{c}^{narr}_i$; for 
\textit{Verb}, $\mathbf{c}{'}_i = \mathbf{c}^{verb}_i$; for \textit{Verb+Context}, $\mathbf{c}{'}_i = concat(\mathbf{c}^{verb}_{i-1}, \mathbf{c}^{verb}_i)$; for \textit{Narration+Context}, $\mathbf{c}{'}_i = concat(\mathbf{c}^{narr}_{i-1}, \mathbf{c}^{narr}_i)$. We also run experiments on a subset of all segments by filtering out segments that are less than \textit{Segment Threshold}$\times 64$ frames long (in the original 50fps sampling rate). Moreover, in Table~\ref{table:howto100m_visual_seg_threshold}, we vary $\mathbf{v}^{'}_i$ to see the impact of including visual features from neighboring segments. When $l_v = l$, 
$\mathbf{v}^{'}_i = concat(\{\mathbf{v}_j\}_{j\in [i-l\dots i]})$. 

\subsection{MS-TCN Experiments} \label{section:mstcn-experiments}
We selected the best performing HowTo100M joint embedding model and used it after the prediction of MS-TCN to generate the final prediction, as explained in detail in Section~\ref{section:video-text-matching}. We evaluate the trained joint embedding on action segmentation task by using it as a post-processing mechanism on MS-TCN's predicted output. Selecting the two better performing embedding setting, we evaluate our embedding on \textit{Narration} and \textit{Narration + Context} setting. After obtaining the segments' start-end predictions $(s_i,e_i)_{i \in [N']}$, instead of retrieving the closest narration, we retrieve from word embeddings in the form of 
$conat(\mathbf{c}^{pred}_{i-1}, \mathbf{c}_j)$, where $\mathbf{c}^{pred}_{i-1}$ is the narration that is the closest to the $(i-1)$-th segment in the joint-embedding. Moreover, we skip over segments that are predicted as background by MS-TCN. We show results for both retrieval with the original text input $\mathbf{c}_j$ and the text input with context $conat(\mathbf{c}^{pred}_{i-1})$ in Table~\ref{table:results}. 

\begin{table}[b]
\resizebox{\linewidth}{!}{
\begin{minipage}[c]{\linewidth}
\centering
\begin{tabular}{lrrrr}
\toprule
Label & MR & R1 & R5 & R10 \\
\midrule
Verb & \multicolumn{4}{c}{Fail to learn}  \\
Verb+Context & 132 & 0.01 & 0.03 & 0.05\\
Narration & 40 & 0.04 & 0.14 & 0.22 \\
Narration+Context & 16 &  0.19 & 0.49 & 0.66 \\
\bottomrule
\end{tabular}
\caption{Results of Video-Text Retrieval using different types of label}
\label{table:howto100m_label}
\end{minipage}}
\end{table}
% \clearpage