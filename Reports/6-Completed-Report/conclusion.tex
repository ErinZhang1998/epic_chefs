We presented a multi-modal approach to the action segmentation task. Given the difficulty of the action segmentation task and the EPIC-KITCHENS dataset, we want to produce better prediction by post-processing the baseline model's prediction with a trained visual-textual joint embedding. The baseline model follows a multi-stage temporal convolution architecture, and the joint embedding is trained with max-margin ranking loss. Although our experiments show that the proposed methodology does not improve the prediction, we identified potential caveats through analysis of textual and visual information. We also experimented with ways to extract better visual features using SlowFast network, which can be find in Appendix~\ref{section:visual-feature-analysis}, but even though features from SlowFast network work well on action recognition, feeding them into MS-TCN does not improve baseline performance. Similarly, the video-text retrieval model works well with well-trimmed segments but not so when fed with noisy visual features, so in the future, we would like to investigate more about how visual and text input interact across time, since the additional temporal dimension results in most of the complexity in our experiments.  