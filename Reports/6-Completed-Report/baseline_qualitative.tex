\subsection{Qualitative Analysis and Examples}
% This section should likely contain a table of examples demonstrating how the current approach succeeds/fails.

From \ref{fig:var_baseline} we can see that it is very challenging for FC to get the class label correctly, as it predicts the most common verb ``wash" (olive-green) instead of the ground truth ``mix" (hot pink). For videos with a few number of long segments, DGTRM tends to over-segment. In \ref{fig:mstcn_joint} we see that the joint embedding trained with \textit{Narration+Context} gives a slightly better classification result than one trained only with \textit{Narration}, suggesting richness in textual modality is key.

