% File project.tex
%% Style files for ACL 2021
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{booktabs}
\usepackage[draft]{todonotes}
\usepackage{latexsym}
% \usepackage[demo]{graphicx}
\usepackage{subcaption}
\usepackage[title]{appendix}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{indentfirst}
\usepackage{xcolor}
\usepackage{natbib, multirow}
\usepackage{graphics}
\usetikzlibrary{shapes.geometric}


% \usepackage{subfig}
\renewcommand{\UrlFont}{\ttfamily\small}


% Packages for collaborative annotations
\usepackage{comment}
%\usepackage[draft]{todonotes}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy 

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Improving Action Segmentation on Large Egocentric Cooking Dataset}

\author{
  Yun Cheng\thanks{\hspace{4pt}Everyone Contributed Equally -- Alphabetical order} \hspace{2em} Yuxuan Liu$^*$ \hspace{2em} Tiffany Ma$^*$ \hspace{2em} Erin Zhang$^*$ \\
  \texttt{\{yuncheng, yuxuanli, tma1, xiaoyuz1\}@andrew.cmu.edu}
}

\date{\today}

\begin{document}
\maketitle
\begin{abstract}
The task of action segmentation involves identifying not only the start and end time of different actions in an untrimmed video but also the action types. Previous approaches take in only visual inputs, whereas we attempt to solve the task using additional text input. We test our methods on the EPIC-KITCHENS dataset, whose narration annotations allow us to learn a visual-textual joint-embedding. We build upon the existing MS-TCN model which produces the start and end time of segments in a video, and we uses the visual features of the predicted segment to retrieve the closest narration in terms of their distance in the joint space. Although the video-text retrieval component does not improve baseline performance, we analyze its strength in terms of action recognition and the causes of potential failure cases. 
\end{abstract}

\section{Introduction and Problem Definition}

\input{intro}

% \begin{figure}
% \missingfigure[figwidth=\linewidth]{This is a simple example/demonstration figure that explains your task and insight}
% \end{figure}

%\clearpage
\input{related_work}

%\clearpage

\section{Task Setup and Data}
The main task is to segment egocentric (first-person) cooking videos from EPIC-KITCHENS dataset into action-object pairs. Given a video clip in the form of a sequence of frames, we want to identify the type of actions as well as their start and end time in the given video.


\subsection{Dataset}
% left out the word extended because we never mentioned EPIC-KITCHEN-55 %
We use the largest egocentric (first-person) dataset  EPIC-KITCHENS-100, which features 100 hours, 700 variable-length videos with 90K actions of 37 participants \cite{Damen2020RESCALING}. The egocentric view provides a unique perspective on people-object interactions, attention, and intention. Meanwhile, it also imposes extra challenges compared to third-person datasets like YouCook2 \cite{ZhXuCoAAAI18}. One of the challenges is that certain actions, such as eating and drinking, cannot be directly observed due to the limited field of view. Other challenges include unseen participants, unseen cooking actions, frame noises from different sources (i.e. background and lighting), long videos with many action instances.
%, fragmentation of segments resulted from interleaving actions in multi-tasking, and weaker temporal correlations in objects interfering the correlations in actions.
% Compared to YouTube-based datasets such as HowTo100M \cite{miech19howto100m}, EPIC-KITCHENS contains activities that are non-scripted and thus capture more natural settings such as parallel tasking .

\subsection{Task formulation}
The dataset consists of two modalities: video frames of egocentric cooking scenes and narrations describing the action in the scenes. The narrations are transcribed from the audio in the form of imperative phrases: verb-noun with optional propositional phrase. The goal is to predict a verb class for each frame to identify the action in the segments. 

Formally, the visual input consists of a sequence of $M$ RGB frames in temporal order, denoted as $F=(\mathbf{f}_i)_{i=1}^M$. The RGB frames are sampled from untrimmed videos at a rate of 50 frames per second. The textual input is a sequence of $N$ audio-transcribed narrations in temporal order, denoted as $C=(\mathbf{c}_i)_{i=1}^N$. Our goal is to infer the action class label for each frame. The ground truth is given by $Y=(\mathbf{y}_i)_{i=1}^M$. Each $\mathbf{y}_i\in\{0,1\}^K$ is a tuple of one-hot vectors encoding the true verb class, where $K$ is the number of verb classes.


\input{dataset_statistic.tex}


\subsection{Metrics}
\input{metrics.tex}

\section{Models}

\subsection{Baselines}
\input{baselines}

\subsection{Proposed Approach} \label{section:proposed-approach}
\input{proposed_approach}

% \input{potential_experiment}


\section{Results}
\input{results_section}
\begin{table*}[h!]
\begin{minipage}[b]{1\textwidth}
\centering
\begin{tabular}{lrrrrrr}
\toprule
% & \multicolumn{3}{c}{Test}\\
Methods  & Acc & Edit & F1$@\{10,25,50\}$ \\
\midrule
FC  & 34.90 & 18.58 & 17.47~~13.66~~8.04\\
% EDTCN \cite{8099596} & & & & & & \\
MS-TCN \cite{8953830} & 38.65 & & \\
% RNN+HMM \cite{8099623} & & & & & & \\
DTGRM \cite{wang2020temporal} & 37.71 & & \\
\midrule
Proposed Method (\textit{narration}) & 20.93 & 24.20 & 2.49 & \\
Proposed Method (\textit{narration+context})& 22.81 & 24.40 & 2.69 & \\
\bottomrule
\end{tabular}
\caption{Results of baseline models}
\label{table:results}
\end{minipage}
~\\
\begin{minipage}[b]{1\textwidth}
\centering
\begin{tabular}{lrrrrr}
\toprule
Label &  Segment Threshold ($l_s$) & MR & R1 & R5 & R10 \\
\midrule
\multirow{3}{*}{Narration} & 0 & 46 & 0.03 & 0.11 & 0.18 \\
 & 1 & 40 & 0.04 & 0.14 & 0.22 \\
 & 3 & 13 & 0.10 & 0.30 & 0.45 \\
\midrule
 & 1 & 132 & 0.01 & 0.03 & 0.05 \\
Verb+Context & 3 & 104 & 0.01 & 0.04 & 0.07 \\
 & 5 & 26 & 0.03 & 0.13 & 0.24\\
\midrule
 & 1 & 6 & 0.19 & 0.49 & 0.66 \\
Narration+Context & 3 & 5 & 0.23 & 0.54 & 0.70 \\
 & 5 & 2 & 0.36 & 0.81 & 0.92 \\
\bottomrule
\end{tabular}
\caption{Results of Video-Text Retrieval using different segment threshold}
\label{table:howto100m_seg_threshold}
\end{minipage}
~\\
\begin{minipage}[b]{1\textwidth}
\centering
% \begin{tabular}{lrrrrr}
\begin{tabular}
{c{0.2\linewidth}  c{0.15\linewidth} c{0.08\linewidth} c{0.08\linewidth}  c{0.08\linewidth}  c{0.08\linewidth}}
\toprule
Visual Context Threshold ($l_v$) & Segment Threshold ($l_s$) & MR & R1 & R5 & R10 \\
\midrule
2 & 0 & 135 & 0.01 & 0.03 & 0.07 \\
3 & 0 & 50 & 0.02 & 0.08 & 0.16 \\
4 & 0 & 26 & 0.03 & 0.16 & 0.27 \\
3 & 1 & 36 & 0.03 & 0.14 & 0.23 \\
3 & 3 & 102 & 0.02 & 0.09 & 0.14 \\
\bottomrule
\end{tabular}
\caption{Results of Video-Text Retrieval using different segment threshold on visual features}
\label{table:howto100m_visual_seg_threshold}
\end{minipage}
\end{table*}

\section{Analysis}
\input{baseline_results}

\subsection{Analysis of Text-Video Retrieval} 
\input{howto100m}

% \subsection{Ablations and Their Implications}
\subsection{Analysis of post-MSTCN Text-Video Retrieval} 
\input{ablations}

\input{baseline_qualitative}

% \subsection{Analysis of SlowFast Visual Feature} \label{section:visual-feature-analysis}
% \input{roi_results}


\section{Conclusion}
\input{conclusion}

\newpage
% Please use 
\bibliographystyle{acl_natbib}
\bibliography{references}

\clearpage

\begin{appendices}


\section{Data Analysis}
\label{appendix:A}
\input{appendix-dataset-analysis}

\section{SlowFast Visual Feature} 
\label{section:visual-feature-analysis}
\input{roi_results}

\end{appendices}


\end{document}
